%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
% SUMMARY    : Lecture 6
%            : University of Southern Maine 
%            : @james.quinlan
%            : Ellis Fitzgerald - Lecture 8
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\section*{Objectives}
\begin{outline}
    \1 Regression
    \1 Loss Functions
    \1 Lectures 1-8 Review
    \1 Practice Problems
\end{outline}

\rule[0.0051in]{\textwidth}{0.00025in}
% ----------------------------------------------------------------


\section{Regression}
The previous lectures and materials have mostly covered classification tasks. As we know, classification can be binary where there are two (2) labels: 
\[
y \in \{-1, 1\}
\]

Though, classification tasks can contain $x\mathbb{\in Z^+}$ labels. In a small example, the Iris dataset has three (3) different labels representing the Iris flowers setosa, versicolor, and virginica.
\[
y \in \{1, 2, 3\}
\]
\textbf{Regression} has the same goal of ``labeling'' features, but not categorically and instead with real numbers.
\[
y \in \mathbb{R}
\]
For both regression and classification, the dataset takes the same form:
\[
D = \{(x_1, y_1), (x_2, y_2),...,(x_n, y_n)\}
\]
Where for each feature, there is an associated label.
\section{Loss Functions}
\textbf{Loss} is calculated by a function for supervised machine learning tasks. Its responsibility is to answer the question: 
\[
\text{By how much is the model \textit{wrong}?}
\]

\subsection{Zero-to-One Loss}
\textbf{Loss} functions can be simple and binary, similar to classification tasks. In example, a zero-to-one loss function would be denoted as
\[
l_{01} \in \{0, 1\}
\]
In this function, 1 represents a loss, and 0 represents no loss.

When speaking about loss functions, some may interchangeably call a functions \textbf{Aggregate} (summation) a loss function, but this is more accurately described as being a \textbf{Cost} function.
\[ 
\sum_{i=1}^{n} l_{01}(i) = 1 + 0 + 1 + 1 + 0 . . . 
\]
In this cost function, the higher the value the more loss the model has. Simply put: the higher the score $\implies$ the worse the performance. 

\subsection{Absolute Loss}
\textbf{Absolute Loss} takes the actual label associated with the feature $y_i$ and gets the absolute difference with \footnote{}$\hat{y_i}$ label value.
\footnote[1]{The hat symbol $\hat{}$ implies that this variable is a computed value.}

\[ 
\text{Loss}=|y_i - \hat{y_i}|
\]
\[ 
\text{Cost}=\sum_{i=1}^{n} |y_i - \hat{y_i}|
\]
The downside of the absolute loss function is that it is not \textbf{differentiable}. Recall that differentiable means there exists a derivative.

\subsection{Square Loss}
Square loss takes the actual label associated with the feature $y_i$ and squares its difference with the computed $\hat{y_i}$ label value.
\[ 
\text{Loss}=(y_i - \hat{y_i})^2
\]
\[ 
\text{Cost}=\sum_{i=1}^{n} (y_i - \hat{y_i})^2
\]
Because this loss function squares its result it can be sensitive to outliers. However, because it is differentiable, it is often preferred over the absolute loss function.

\subsection{Objective Function}
Recall that we covered how a \textbf{Cost} function is the aggregation of a \textbf{Loss} function.
\[ 
\text{Cost}=\sum_{i=1}^{n} \text{Loss}
\]
Similarly, there is another function known as the \textbf{Objective Function} which is the sum of the cost and a \textbf{Regularizer}:
\[ 
\text{Objective Function}=\text{Cost} + \text{Regularizer}
\]
The purpose of the regularizer is to add a penalty to shrink a coefficient. This can be helpful for loss functions like square loss as it can be sensitive to outliers and compute unexpectedly large coefficients.

\subsection{Gradient Descent}
Gradient descent is essentially a use case for calculating derivatives. Its purpose in Supervised Machine Learning (ML) tasks is to find local minimum and maximums (where the derivative is 0) in order to optimize the loss function of a model. 

Here are the names of some notable gradient descent methods:
\begin{outline}
    \1 Vinalla
    \1 Stochastic
    \1 Minibatch
    \1 Hession
    \1 Momentum

\end{outline}
% ----------------------------------------------------------------
\section{Lectures 1-7 Review}

\subsection{The Perceptron}
% List of quick facts %
\begin{outline}
    \1 Developed by \textbf{Rosenblatt} in \textbf{1957}
    \1 \textbf{Binary Classification} Task
    \1 Assumes data is \textbf{Linearly Separable}
    \1 Calculates weight vector $\vec{w}$ and bias $b$ 
    \1 ``Bundle \& Save''   
    \begin{align*}
    \vec{w} &= \begin{bmatrix}
           w_{1}    \\
           w_{2}    \\
           \vdots   \\
           w_{p}    \\
           b        \\
         \end{bmatrix} \\
    \end{align*}
    \1 $\vec{w} \in \mathcal{H}$ where $\mathcal{H}$ \textbf{Decision Boundary} or \textbf{Hyperplane}
\end{outline}

\[
\mathcal{H} = \{\vec{x} | \vec{w}^T\vec{x} + b = 0\}
\]


\subsection{KNN}
\begin{outline}
    \1 Stands for \textbf{$k$ Nearest Neighbors}
    \1 Loops over all neighbors and calculates distance
    \1 Sorts distances and returns $k$ nearest
    \1 \textbf{Regression:} Averages values to nearest neighbor values
    \1 \textbf{Classification:} Sets label to the argmax of nearest neighbor values
\end{outline}

\begin{verbatim}
w = 0
m = 1
while m > 0:
    m = 0
    for i = 1..n:
        if y[i] * dot(w, x[i]) <= 0:
            w = w + alpha * y[i] * x[i]
            m = m + 1
        end
    end
end
\end{verbatim}

% ----------------------------------------------------------------
\section{Practice Problems}

\begin{outline}[enumerate]
    \1 What is the benefit of having less dimensions, features, or components in your vectors?
    \1 What are the different types of Supervised ML?
    \1 What is a loss function?
    \1 What is KNN? How could it be used differently for both Classification and Regression?
    \1 What does SVM stand for? What is its purpose?
    \1 What is the definition of a distance function? Give examples.
\end{outline}